{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "215fd586-7464-40ea-a823-e9a58ab2248e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92c0d7c-d7c3-4fe1-a493-4fecd5ed227e",
     "showTitle": false,
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "DB_NAME = 'db_demo_rh'\n",
    "DB_USER = ''\n",
    "DB_PASSWORD = ''\n",
    "DB_HOST = ''\n",
    "DB_PORT = '5432'\n",
    "SCHEMA = ''\n",
    "\n",
    "S3_BUCKET = '/Volumes/path/to/volume/'\n",
    "TEMP_DIR = '/Volumes/path/to/volume/tmp/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60a7461-c4cf-48bd-b374-f3549335dbb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56d66743-b70f-4e43-8e85-d732951ab25b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sqlalchemy import create_engine, inspect\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import binascii\n",
    "import io\n",
    "%pip install openpyxl xlrd\n",
    "\n",
    "def get_tables():\n",
    "    engine = get_engine()\n",
    "    inspector = inspect(engine)\n",
    "    tables = inspector.get_table_names(schema=SCHEMA)\n",
    "    engine.dispose()\n",
    "    return tables\n",
    "\n",
    "\n",
    "def get_table_columns(table_name):\n",
    "    engine = get_engine()\n",
    "    inspector = inspect(engine)\n",
    "    columns = [col['name'] for col in inspector.get_columns(table_name, schema=SCHEMA)]\n",
    "    engine.dispose()\n",
    "    return columns\n",
    "\n",
    "\n",
    "def get_engine():\n",
    "    connection_string = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "    engine = create_engine(connection_string)\n",
    "    return engine\n",
    "\n",
    "\n",
    "def load_csv_to_table(df, table_name):\n",
    "    engine = get_engine()\n",
    "    df.to_sql(table_name, engine, schema=SCHEMA, if_exists='append', index=False)\n",
    "    print(f\"Records inserted to the table: {table_name}\")\n",
    "\n",
    "\n",
    "\n",
    "def decode_and_extract_zip(binary_data):\n",
    "    with zipfile.ZipFile(io.BytesIO(binary_data)) as zip_ref:\n",
    "        extracted_files = {}\n",
    "        for file_info in zip_ref.infolist():\n",
    "            with zip_ref.open(file_info) as file:\n",
    "                extracted_files[file_info.filename] = file.read().decode('utf-8')\n",
    "        return extracted_files\n",
    "\n",
    "\n",
    "def bulk_import():\n",
    "    tables = get_tables()\n",
    "    for table in tables:\n",
    "        binary_path = f\"{S3_BUCKET}{table}.csv\"\n",
    "        try:\n",
    "            \n",
    "            with open(binary_path, 'rb') as binary_file:\n",
    "                binary_data = binary_file.read()\n",
    "\n",
    "            \n",
    "            df = pd.read_excel(io.BytesIO(binary_data), header=None)\n",
    "\n",
    "            columns = get_table_columns(table)\n",
    "            df.columns = columns\n",
    "\n",
    "            load_csv_to_table(df, table)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"File {binary_path} not found, skipped\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error when processing the file {binary_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aaf4190-edd7-404e-8ae5-6c09bf6e000b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "956f15b9-589a-41ba-9e71-168f5dfd890f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bulk_import()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Import_to_postgres_from_csv",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
